{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "spacy_en = en_core_web_sm.load()\n",
    "spacy_de = de_core_news_sm.load()\n",
    "\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    \"\"\"The encoder stack of NAT.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed=512, nhead=8, num_encoder_layers=6,\n",
    "                 dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        \n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_embed, nhead, dim_feedforward, dropout, activation)\n",
    "        encoder_norm = nn.LayerNorm(d_embed)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "        \n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "            src: [S, N, E]\n",
    "            output: [S, N, E]\n",
    "        \"\"\"\n",
    "        output = self.encoder(src, mask, src_key_padding_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 20\n",
    "L = 50\n",
    "d_embed=512\n",
    "nhead=8\n",
    "num_encoder_layers=6\n",
    "num_decoder_layers=6\n",
    "dim_feedforward=2048\n",
    "dropout=0.1\n",
    "activation=\"relu\"\n",
    "\n",
    "encoder = EncoderStack(d_embed, nhead, num_encoder_layers, dim_feedforward, dropout, activation)\n",
    "fertility_predictor = FertilityPredictor(d_embed, L)\n",
    "position_encoder_en = PositionalEncoding(d_embed, S)\n",
    "position_encoder_de = PositionalEncoding(d_embed, S*L)\n",
    "decoder = DecoderStack(d_embed, nhead, num_decoder_layers, dim_feedforward, dropout, activation)\n",
    "\n",
    "input_e = torch.rand(20, 16, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderStack(d_embed, nhead, num_decoder_layers, dim_feedforward, dropout, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 64000000 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-56b7defb847f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcopied_embedding_pe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition_encoder_de\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopied_embedding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# copied_embedding_pe: [T, N, E]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdecoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopied_embedding_pe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# decoder_output: [T, N, E]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-d7738b91d167>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m     20\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[0;32m     21\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                               memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    226\u001b[0m                                     \u001b[0mmemory_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                                     \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                                     memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-a06b58590ea6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# multi-head self-attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mtgt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_head_self_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m    817\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 attn_mask=attn_mask)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   3382\u001b[0m         \u001b[1;31m# average attention weights over heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3383\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3385\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3386\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 64000000 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "input_pe = position_encoder_en(input_e)\n",
    "encoder_output = encoder(input_e)\n",
    "fertility_list = fertility_predictor(encoder_output)\n",
    "copied_embedding = NAT.copy_fertility(input_e, fertility_list, L)\n",
    "copied_embedding_pe = position_encoder_de(copied_embedding) # copied_embedding_pe: [T, N, E]\n",
    "memory = encoder_output\n",
    "decoder_output = decoder(copied_embedding_pe, memory) # decoder_output: [T, N, E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 16, 512])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copied_embedding_pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_input = nn.Embedding(vocab_src, d_embed)\n",
    "position_encoder_en = PositionalEncoding(d_embed, S)\n",
    "position_encoder_de = PositionalEncoding(d_embed, S*L)\n",
    "encoder = EncoderStack(d_embed, nhead, num_encoder_layers, dim_feedforward, dropout, activation)\n",
    "fertility_predictor = FertilityPredictor(d_embed, L)\n",
    "decoder = DecoderStack(d_embed, nhead, num_decoder_layers, dim_feedforward, dropout, activation)\n",
    "translation_predictor = TranslationPredictor(d_embed, vocab_tgt)\n",
    "\n",
    "\n",
    "input_e = self.embedding_input(input.transpose(0,1)) # input_e: [S, N, E]\n",
    "input_pe = self.position_encoder_en(input_e) # input_pe: [S, N, E]\n",
    "encoder_output = self.encoder(input_e) # encoder_output: [S, N, E] ---------------\n",
    "fertility_list = self.fertility_predictor(encoder_output) # fertility_list: [S, N, 1]\n",
    "copied_embedding = copy_fertility(input_e, fertility_list, self.L) # copied_embedding: [T, N, E]\n",
    "copied_embedding_pe = self.position_encoder_de(copied_embedding) # copied_embedding_pe: [T, N, E]\n",
    "memory = encoder_output\n",
    "decoder_output = decoder(copied_embedding_pe, memory) # decoder_output: [T, N, E]\n",
    "output = translation_predictor(decoder_output) # output: [T, N, E]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 16, 512])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copied_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding.\n",
    "    \n",
    "    Args:\n",
    "        d_embed: int, the dimension of embedding.\n",
    "        max_seq_len: int, the max length of what to be summed with positional encoding.\n",
    "    \n",
    "    \"\"\"\n",
    "    def  __init__(self, d_embed, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_embed = d_embed\n",
    "        pe = torch.zeros(max_seq_len, d_embed) # pe: [T, E]\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_embed, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_embed)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_embed)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0) # pe: [1, T, E]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_embed) # x: [T, N, E]\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        pe = self.pe[:, :seq_len].repeat(x.size(1),1,1).transpose(0,1) # pe: [T, N, E]\n",
    "        pe = Variable(pe, requires_grad=False).cuda() \\\n",
    "            if x.is_cuda else Variable(pe, requires_grad=False)\n",
    "        x = x + pe\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadPositionalAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Positional Attention sublayer.\"\"\"\n",
    "    def __init__(self, d_embed, num_heads, dropout=0., bias=True, add_bias_kv=False,\n",
    "                 add_zero_attn=False, kdim=None, vdim=None, max_seq_len=100):\n",
    "        super(MultiHeadPositionalAttention, self).__init__()\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_embed, max_seq_len)\n",
    "    \n",
    "        self.multi_head_attn = nn.MultiheadAttention(d_embed, num_heads, dropout, bias, \n",
    "                                                     add_bias_kv, add_zero_attn, kdim, vdim)\n",
    "        \n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, \n",
    "                attn_mask=None):\n",
    "        query = self.positional_encoding(query) # query: [L, N, E]\n",
    "        key = self.positional_encoding(key) # key: [S, N, E]\n",
    "        value = self.positional_encoding(value) # value: [S, N, E], L = S\n",
    "        \n",
    "        # attn_output: [L, N, E], attn_output_weights: [N, L, S]\n",
    "        attn_output, _ = self.multi_head_attn(query, key, value, key_padding_mask, \n",
    "                                                                need_weights, attn_mask)\n",
    "        \n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"The sublayer of DecoderStack.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed, nhead, dim_feedforward=2048, max_seq_len=100, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_self_attn = nn.MultiheadAttention(d_embed, nhead, dropout)\n",
    "        self.multi_head_pos_attn = MultiHeadPositionalAttention(d_embed, nhead, dropout)\n",
    "        self.multi_head_inter_attn = nn.MultiheadAttention(d_embed, nhead, dropout)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_embed, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_embed)\n",
    "\n",
    "        # Implementation of MultiHead-Attentions\n",
    "        self.norm1 = nn.LayerNorm(d_embed)\n",
    "        self.norm2 = nn.LayerNorm(d_embed)\n",
    "        self.norm3 = nn.LayerNorm(d_embed)\n",
    "        self.norm4 = nn.LayerNorm(d_embed)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "\n",
    "        self.d_embed = d_embed\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"DecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "        \n",
    "        Shape:\n",
    "            tgt: [T, N, E].\n",
    "            memory: [S, N, E]\n",
    "        \"\"\"\n",
    "        tgt_len, bsz = tgt.size(0), tgt.size(1)\n",
    "        \n",
    "        # define tgt_mask\n",
    "        if tgt_mask is None:\n",
    "            diag_ones = np.array([1]*tgt_len)\n",
    "            tgt_mask = torch.from_numpy(np.diag(diag_ones)).byte()\n",
    "            tgt_mask = tgt_mask.float().masked_fill(tgt_mask, float('-inf'))\n",
    "        \n",
    "        # multi-head self-attention\n",
    "        tgt2 = self.multi_head_self_attn(tgt, tgt, tgt)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # multi-head positional attention\n",
    "        tgt2 = self.multi_head_pos_attn(tgt, tgt, tgt)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # multi-head inter-attention\n",
    "        tgt2 = self.multi_head_inter_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        # position-wise feed forward layer\n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm4(tgt)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    \"\"\"The decoder stack of NAT.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed=512, nhead=8, num_decoder_layers=6, \n",
    "                 dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        \n",
    "        decoder_layer = DecoderLayer(d_embed, nhead, dim_feedforward, dropout)\n",
    "        decoder_norm = nn.LayerNorm(d_embed)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "            tgt: [T, N, E]\n",
    "            memory: [S, N, E]\n",
    "            output: [T, N, E]\n",
    "        \"\"\"\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FertilityPredictor(nn.Module):\n",
    "    \"\"\"The fertility predictor of NAT.\n",
    "    \n",
    "    Args:\n",
    "        d_embed: the dimension of EncoderStack's output.\n",
    "        L: the number of the classes used to represent fertility.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed, L):\n",
    "        super(FertilityPredictor, self).__init__()\n",
    "        \n",
    "        self.fc_layer = nn.Linear(d_embed, L)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, encoder_output):\n",
    "        \"\"\"Using EncoderStack's output to predict fertility list.\n",
    "        \n",
    "        Shape:\n",
    "            encoder_output: [S, N, E]\n",
    "            fertility_list: [S, N, 1]\n",
    "        \"\"\"\n",
    "        fertility_list = F.softmax(self.relu(self.fc_layer(encoder_output)), dim=-1) # fertility_list: [S, N, L]\n",
    "        fertility_list = torch.argmax(fertility_list, dim=-1) # fertility_list: [S, N, 1]\n",
    "        \n",
    "        return fertility_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPredictor(nn.Module):\n",
    "    \"\"\"The translation predictor of NAT.\n",
    "\n",
    "    Args:\n",
    "        d_embed: the dimension of EncoderStack's output.\n",
    "        vocab: the number of the classes used to represent fertility.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed, vocab):\n",
    "        super(TranslationPredictor, self).__init__()\n",
    "        \n",
    "        self.fc_layer = nn.Linear(d_embed, vocab)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, decoder_output):\n",
    "        \"\"\"Using DecoderStack's output to predict translation.\n",
    "        \n",
    "        Shape:\n",
    "            decoder_output: [S, N, E]\n",
    "            translation_output: [S, N, vocab]\n",
    "        \"\"\"\n",
    "        translation_output = F.softmax(self.relu(self.fc_layer(decoder_output)), dim=-1)\n",
    "        \n",
    "        return translation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAT(nn.Module):\n",
    "    \"\"\"Non-Autoregressive Transformer.\n",
    "    \n",
    "    Args:\n",
    "        vocab_src: int, the size of source vocabulary.\n",
    "        vocab_tgt: int, the size of target vocabulary.\n",
    "        d_embed: int, the dimension of embedded input.\n",
    "        S: int, the length of source input sentence.\n",
    "        L: int, the number of the classes used to represent fertility.\n",
    "        \n",
    "    Shape:\n",
    "        input: LongTensor, [N, S]\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_src, vocab_tgt, S, d_embed=512, L=50, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(NAT, self).__init__()\n",
    "        \n",
    "        self.d_embed = d_embed\n",
    "        self.S = S\n",
    "        self.L = L\n",
    "        self.T = S*L\n",
    "        \n",
    "        self.embedding_input = nn.Embedding(vocab_src, d_embed)\n",
    "        self.position_encoder_en = PositionalEncoding(d_embed, S)\n",
    "        self.position_encoder_de = PositionalEncoding(d_embed, S*L)\n",
    "        self.encoder = EncoderStack(d_embed, nhead, num_encoder_layers, dim_feedforward, dropout, activation)\n",
    "        self.fertility_predictor = FertilityPredictor(d_embed, L)\n",
    "        self.decoder = DecoderStack(d_embed, nhead, num_decoder_layers, dim_feedforward, dropout, activation)\n",
    "        self.translation_predictor = TranslationPredictor(d_embed, vocab_tgt)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_e = self.embedding_input(input.transpose(0,1)) # input_e: [S, N, E]\n",
    "        input_pe = self.position_encoder_en(input_e) # input_pe: [S, N, E]\n",
    "        encoder_output = self.encoder(input_e) # encoder_output: [S, N, E] ---------------\n",
    "        fertility_list = self.fertility_predictor(encoder_output) # fertility_list: [S, N, 1]\n",
    "        copied_embedding = copy_fertility(input_e, fertility_list, self.L) # copied_embedding: [T, N, E]\n",
    "        copied_embedding_pe = self.position_encoder_de(copied_embedding) # copied_embedding_pe: [T, N, E]\n",
    "#         copied_memory = copy_fertility(encoder_output, fertility_list, self.L) # copied_memory: [T, N, E]\n",
    "        memory = encoder_output\n",
    "        decoder_output = self.decoder(copied_embedding_pe, memory) # decoder_output: [T, N, E]\n",
    "        output = self.translation_predictor(decoder_output) # output: [T, N, E]\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def copy_fertility(input_e, fertility_list, L):\n",
    "        \"\"\"Copy the input embedding as the number at corresponding index.\n",
    "        \n",
    "        Args:\n",
    "            input_e: [S, N, E].\n",
    "            fertility_list: [S, N, 1].\n",
    "            L: int, the number of the classes used to represent fertility.\n",
    "        \n",
    "        Output:\n",
    "            copied_embedding: [T, N, E]\n",
    "        \"\"\"\n",
    "        # copy as fertitlity list\n",
    "        [S, N, E] = input_e.shape\n",
    "        copied_embedding = torch.zeros(N, S*L, E) # copied_embedding: [N, S*L, E]\n",
    "        input_e_permute = input_e.permute(1,0,2) # input_e_permute: [N, S, E]\n",
    "        fertility_list_permute = fertility_list.transpose(0,1) # fertility_list_permuet: [N, S]\n",
    "        \n",
    "        # use fertility list and embedded input to get decoder's input.\n",
    "        for i, fertility_batch in enumerate(fertility_list_permute):\n",
    "            pos = 0\n",
    "            for j, fertility_j in enumerate(fertility_batch):\n",
    "                copied_embedding[i,pos:pos+int(fertility_j),:] = input_e_permute[i,j,:].repeat(1,int(fertility_j),1)\n",
    "                pos += int(fertility_j)\n",
    "        copied_embedding = copied_embedding.transpose(0,1)\n",
    "        \n",
    "        return copied_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(0, 1000, (16, 20)).long() # input: [N, S]\n",
    "model = NAT(vocab_src=1000, vocab_tgt=500, S=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[573, 406, 183, 764,  73, 235, 182, 767, 183, 476, 783,  57, 530, 539,\n",
       "          53, 406, 378, 960, 343, 514],\n",
       "        [408,  88, 195, 370, 323, 726, 616, 317, 771,  76, 434, 677, 257,  18,\n",
       "         460, 886, 867, 318, 723, 438],\n",
       "        [410, 411, 566, 770,  37, 708,  97, 196, 433, 811, 613, 589, 615, 330,\n",
       "         187, 966, 210, 263, 684, 809],\n",
       "        [984, 658, 301, 520, 954, 545, 247, 414, 924, 996, 725, 635, 625, 615,\n",
       "         330, 873, 526, 329, 451, 647],\n",
       "        [592, 854,  77, 300, 840, 543, 569, 292, 727, 440, 578, 638, 871, 832,\n",
       "         776, 869, 969, 573, 352,   0],\n",
       "        [ 39, 603,  56,  21, 596, 600, 910, 513, 913, 873, 224, 227, 325, 409,\n",
       "         775, 450, 863, 581, 900, 662],\n",
       "        [651,  44, 171, 335, 600, 798, 373, 754, 321, 788,   7, 170, 460, 524,\n",
       "         200, 787, 224, 698, 691, 283],\n",
       "        [355, 207, 968, 217, 557,  24, 380, 735, 853,  75, 169, 223, 191, 104,\n",
       "         679, 919, 465,  73, 655, 167],\n",
       "        [718, 901, 347, 563, 209, 911, 245, 822, 832, 911, 338, 156, 695,  19,\n",
       "         274,  53, 280, 993, 573, 565],\n",
       "        [146,  82, 240, 597, 273,   1, 135,  66, 576, 326, 735, 746, 503, 450,\n",
       "         704, 828, 713, 668, 791, 381],\n",
       "        [310, 409,  83, 162, 242, 826, 428, 571,  87, 569, 981, 921, 965,  25,\n",
       "          25, 155, 791, 321, 552, 554],\n",
       "        [737, 676, 865, 690, 856, 306, 549, 642, 603, 345, 820, 373, 405, 655,\n",
       "           8, 802, 296, 773, 287, 465],\n",
       "        [  2,  62, 708,  23, 167, 569, 120, 353,  55, 437, 445, 566, 107, 228,\n",
       "          43, 736, 319, 253, 315, 497],\n",
       "        [535, 188, 644, 902, 286, 653, 498, 169, 960, 747, 974, 277,  36, 110,\n",
       "         775, 351, 386, 189,  43, 319],\n",
       "        [402, 244, 815, 283, 356,  62, 144,  94,  16, 865, 178, 818, 680, 364,\n",
       "         730, 399, 962, 764, 843, 979],\n",
       "        [991, 755, 807, 923, 920, 745,  62, 622, 960, 753, 992, 370, 114, 353,\n",
       "         303, 226, 663, 870, 928, 690]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
