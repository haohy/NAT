{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "spacy_en = en_core_web_sm.load()\n",
    "spacy_de = de_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention block.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_k, d_q, d_v, d_temp, dropout=0.0):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.k_linear = nn.Linear(d_k, d_temp)\n",
    "        self.q_linear = nn.Linear(d_q, d_temp)\n",
    "        self.v_linear = nn.Linear(d_v, d_v)\n",
    "        self.layer_norm = nn.LayerNorm(d_v)\n",
    "        \n",
    "        self.scale = d_v**(-0.5)\n",
    "        \n",
    "    def forward(self, k, q, v, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "            k: [B, S, d_k];\n",
    "            q: [B, L, d_q];\n",
    "            v: [B, S, d_v];\n",
    "            attn_mask: [L, S], L is the target sequence length, S is the source sequence\n",
    "        length, optional.\n",
    "        \"\"\"\n",
    "        residual = q # residual: [B, S, d_v]\n",
    "        \n",
    "        # Linear before scaled dot-product attntion\n",
    "        _k = self.k_linear(k)\n",
    "        _q = self.q_linear(q)\n",
    "        _v = self.v_linear(v)\n",
    "        \n",
    "        # scaled dot-product attntion\n",
    "        attention = torch.bmm(_k, _q.transpose(1,2)) * self.scale # attention: [B, S, L]\n",
    "        \n",
    "        # mask\n",
    "        if attn_mask is not None: \n",
    "            attention = attention.masked_fill_(attn_mask, -np.inf)\n",
    "            \n",
    "        attn_weight = F.softmax(attention, dim=1) # attn_weight: [B, S, L]\n",
    "        output = torch.bmm(attn_weight.transpose(1,2), v) # output: [B, L, d_embed]\n",
    "        \n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(output + residual)\n",
    "        \n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention, based on SelfAttention.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_input, d_temp, nhead, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.nhead = nhead\n",
    "        d_k, d_q, d_v = d_input, d_input, d_input\n",
    "        self.self_attn_list = nn.ModuleList(SelfAttention(d_k, d_q, d_v, d_temp) for _ in range(nhead))\n",
    "        self.fc_layer = nn.Linear(d_input*nhead, d_input)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, k, q, v, attn_mask=None):\n",
    "        output_list, attn_weight_list = [], []\n",
    "        for i in range(self.nhead):\n",
    "            # # output: [B, L, d_embed], attn_weight: [B, S, L]\n",
    "            if attn_mask is not None:\n",
    "                output_list_i, attn_weight_i = self.self_attn_list[i](k, q, v, attn_mask)\n",
    "            else:\n",
    "                output_list_i, attn_weight_i = self.self_attn_list[i](k, q, v)\n",
    "            output_list_i, attn_weight_i = self.self_attn_list[i](k, q, v)\n",
    "            output_list.append(output_list_i)\n",
    "            attn_weight_list.append(attn_weight_i)\n",
    "        output_concat = torch.cat(output_list, dim=-1) # output_concat: [B, L, d_embed*nhead]\n",
    "        output = self.dropout(self.activation(self.fc_layer(output_concat))) # output: [B, L, d_embed]\n",
    "        \n",
    "        return output, attn_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed forward in Encoder and Decoder layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_input, d_output, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Conv1d(d_input, d_ff, 1)\n",
    "        self.linear_2 = nn.Conv1d(d_ff, d_input, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(d_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, L, d_embed]\n",
    "        \"\"\"\n",
    "        # feed forward\n",
    "        output = self.linear_2(self.relu(self.linear_1(x)))\n",
    "        \n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(output + x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder sub-layer.\n",
    "    Args:\n",
    "        d_input: the dimension of embedded input.\n",
    "        d_output: the dimensiof of output.\n",
    "        d_temp: the dimension of multi-head linear's output.\n",
    "        nhead: the number of heads in the multiheadattention models (default=8).\n",
    "        d_ff: the dimension of the feedforward network model (default=2048).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_input, d_output, d_temp=2048, nhead=8, d_ff=2048, dropout=0.0):\n",
    "        \n",
    "        self.multi_head_attn = MultiHeadAttention(d_input, d_temp, nhead, dropout)\n",
    "        self.feed_forward = FeedForward(d_input, d_output, d_ff)\n",
    "        \n",
    "    def forward(self, input, attn_mask=None):\n",
    "        output_attn, attn_weight = self.multi_head_attn(input, input, input, attn_mask)\n",
    "        output = self.feed_forward(output_attn)\n",
    "        \n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    r\"\"\"The encoder stack.\n",
    "    \n",
    "    Args:\n",
    "        d_input: the dimension of embedded input.\n",
    "        d_output: the dimensiof of output.\n",
    "        d_temp: the dimension of multi-head linear's output.\n",
    "        num_encoder: the number of encoder layers.\n",
    "        nhead: the number of heads in the multiheadattention models (default=8).\n",
    "        d_ff: the dimension of the feedforward network model (default=2048).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_input, d_output, d_temp=2048, num_encoder=6, nhead=8, d_ff=2048,\n",
    "                dropout=0.0):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        \n",
    "        self.num_encoder = num_encoder\n",
    "        self.encoder_stack = nn.Sequential(\n",
    "            EncoderLayer(d_input, d_output, d_temp, nhead, d_ff, dropout) \n",
    "            for _ in range(num_encoder))\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def forward(self, input_embedding, mask=None):\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequnce to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "\n",
    "        Shape:\n",
    "            - src: :math:`(S, N, E)`.\n",
    "        \"\"\"\n",
    "        output = input_embedding\n",
    "        for i in range(self.num_encoder):\n",
    "            output, _ = self.encoder_stack[i](output, mask)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the EncoderStack model.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoder sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_input, d_output, d_temp=2048, nhead=8, d_ff=2048, dropout=0.0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.masked_multi_head_attn = MultiHeadAttention(d_input, d_temp, nhead, dropout)\n",
    "        self.multi_head_attn = MultiHeadAttention(d_input, d_temp, nhead, dropout)\n",
    "        self.feed_forward = FeedForward(d_input, d_output, d_ff)\n",
    "        \n",
    "    def forward(self, input, encoder_output, mask=None):\n",
    "        output_1, _ = self.masked_multi_head_attn(input, mask)\n",
    "        output_2, _ = self.multi_head_attn(encoder_output, encoder_output, output_1)\n",
    "        output = self.feed_forward(output_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    r\"\"\"In order to translate non-autoregressively and parallelize the decoding process, \n",
    "    The decoder is different with the original Transformer network. Mainly in the following\n",
    "    aspects:\n",
    "        1. mask out each query position only from attending to itself.\n",
    "        \n",
    "    Args:\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_input, d_output, d_temp=2048, num_decoder=6, nhead=8, d_ff=2048,\n",
    "                dropout=0.1):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        \n",
    "        self.num_decoder = num_decoder\n",
    "        self.decoder_stack = nn.Sequential(\n",
    "            DecoderLayer(d_input, d_output, d_temp, nhead, d_ff, dropout)\n",
    "            for _ in range(num_decoder))\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def forward(self, output_embedding, encoder_output, mask=None):\n",
    "        output = output_embedding\n",
    "        for i in range(self.num_decoder):\n",
    "            output = self.decoder_stack[i](output, encoder_output, mask)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the DecoderStack model.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-0f063d199bfb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-89-0f063d199bfb>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def __init__(self)\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_embed, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        position_encoding = np.array([\n",
    "          [pos / np.pow(10000, 2.0 * (j // 2) / d_embed) for j in range(d_embed)]\n",
    "          for pos in range(max_seq_len)])\n",
    "\n",
    "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
    "\n",
    "        # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding\n",
    "        # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似\n",
    "        # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，\n",
    "        # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码\n",
    "        pad_row = torch.zeros([1, d_model])\n",
    "        position_encoding = torch.cat((pad_row, position_encoding))\n",
    "        \n",
    "        # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，\n",
    "        # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似\n",
    "        self.position_encoding = nn.Embedding(max_seq_len + 1, d_model)\n",
    "        self.position_encoding.weight = nn.Parameter(position_encoding,\n",
    "                                                     requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-a3320a7572e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdiag_ones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiag_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtgt_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "diag_ones = np.array([1]*10)\n",
    "tgt_mask = np.diag(diag_ones)\n",
    "tgt_mask = tgt_mask.float().masked_fill(tgt_mask, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAT(nn.Module):\n",
    "    \"\"\"Non-autoregressive transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "diag_ones = torch.ones((q.size(0), q.size(1))) # diag_ones: [B, L]\n",
    "attn_mask = torch.diag_embed(diag_ones).byte() # attn_mask: [B, L, L]\n",
    "\n",
    "diag_ones = torch.ones((2, 5))\n",
    "mask = torch.diag_embed(diag_ones).byte()\n",
    "# attn_mask = torch.from_numpy(mask).byte()\n",
    "attn_weight = torch.rand(2, 5, 5) # [B, S, L]\n",
    "# attn_mask = attn_mask.repeat(2, 1, 1)\n",
    "attn_weight = attn_weight.masked_fill_(mask, -np.inf)\n",
    "attn_weight = F.softmax(attn_weight, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-inf, 0., 0., 0., 0.],\n",
       "         [0., -inf, 0., 0., 0.],\n",
       "         [0., 0., -inf, 0., 0.],\n",
       "         [0., 0., 0., -inf, 0.],\n",
       "         [0., 0., 0., 0., -inf]],\n",
       "\n",
       "        [[-inf, 0., 0., 0., 0.],\n",
       "         [0., -inf, 0., 0., 0.],\n",
       "         [0., 0., -inf, 0., 0.],\n",
       "         [0., 0., 0., -inf, 0.],\n",
       "         [0., 0., 0., 0., -inf]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.float().masked_fill(mask, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([[1 for _ in range(4)] for _ in range(3)])\n",
    "bt = torch.from_numpy(mask).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt.masked_fill?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FertilityPredictor(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPredictor(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
