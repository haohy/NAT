{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot-product attention mechanism.\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dropout=0.0):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
    "        \"\"\"前向传播.\n",
    "\n",
    "        Args:\n",
    "            q: Queries张量，形状为[B, L_q, D_q]\n",
    "            k: Keys张量，形状为[B, L_k, D_k]\n",
    "            v: Values张量，形状为[B, L_v, D_v]，一般来说就是k\n",
    "            scale: 缩放因子，一个浮点标量\n",
    "            attn_mask: Masking张量，形状为[B, L_q, L_k]\n",
    "\n",
    "        Returns:\n",
    "            上下文张量和attetention张量\n",
    "        \"\"\"\n",
    "        attention = torch.bmm(q, k.transpose(1, 2))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        if attn_mask:\n",
    "            # 给需要mask的地方设置一个负无穷\n",
    "            attention = attention.masked_fill_(attn_mask, -np.inf)\n",
    "            # 计算softmax\n",
    "            attention = self.softmax(attention)\n",
    "            # 添加dropout\n",
    "            attention = self.dropout(attention)\n",
    "        # 和V做点积\n",
    "        context = torch.bmm(attention, v)\n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim=512, num_heads=8, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_k = nn.Linear(self.dim_per_head, self.dim_per_head)\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "\n",
    "        self.dot_product_attention = ScaledDotProductAttention(dropout)\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # multi-head attention之后需要做layer norm\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, attn_mask=None):\n",
    "        # 残差连接\n",
    "        residual = query\n",
    "\n",
    "        dim_per_head = self.dim_per_head\n",
    "        num_heads = self.num_heads\n",
    "        batch_size = key.size(0)\n",
    "\n",
    "        # linear projection\n",
    "        key = self.linear_k(key)\n",
    "        value = self.linear_v(value)\n",
    "        query = self.linear_q(query)\n",
    "\n",
    "        # split by heads\n",
    "        key = key.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        value = value.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "\n",
    "        if attn_mask:\n",
    "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
    "        # scaled dot product attention\n",
    "        scale = (key.size(-1) // num_heads) ** -0.5\n",
    "        context, attention = self.dot_product_attention(\n",
    "          query, key, value, scale, attn_mask)\n",
    "\n",
    "        # concat heads\n",
    "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
    "\n",
    "        # final linear projection\n",
    "        output = self.linear_final(context)\n",
    "\n",
    "        # dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(residual + output)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(input, weight, bias=None):\n",
    "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
    "    r\"\"\"\n",
    "    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
    "\n",
    "    Shape:\n",
    "\n",
    "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
    "          additional dimensions\n",
    "        - Weight: :math:`(out\\_features, in\\_features)`\n",
    "        - Bias: :math:`(out\\_features)`\n",
    "        - Output: :math:`(N, *, out\\_features)`\n",
    "    \"\"\"\n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
